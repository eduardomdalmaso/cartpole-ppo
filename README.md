# CartPole Reinforcement Learning Project

Um projeto de aprendizado por reforÃ§o que implementa agentes para resolver o problema do CartPole usando a biblioteca Gymnasium e Stable Baselines3.

## ğŸ“‹ DescriÃ§Ã£o

Este projeto treina um agente de inteligÃªncia artificial para balancear um poste (pole) em cima de um carrinho (cart) usando tÃ©cnicas de aprendizado por reforÃ§o. O objetivo Ã© manter o poste em pÃ© pelo mÃ¡ximo de tempo possÃ­vel.

## ğŸ¯ Objetivos

- Treinar um agente PPO (Proximal Policy Optimization) para resolver o problema CartPole
- Registrar mÃ©tricas de treinamento usando MLflow
- Avaliar o desempenho do agente treinado
- Armazenar modelos treinados para uso futuro

## ğŸ“ Estrutura do Projeto

```
â”œâ”€â”€ train_cartpole.py      # Script principal de treinamento com MLflow
â”œâ”€â”€ agent-cart.py          # Script de demonstraÃ§Ã£o com aÃ§Ã£o aleatÃ³ria
â”œâ”€â”€ requeriments.txt       # DependÃªncias do projeto
â”œâ”€â”€ README.md              # Este arquivo
â”œâ”€â”€ mlruns/                # HistÃ³rico de execuÃ§Ãµes do MLflow
â”œâ”€â”€ models/                # Modelos treinados armazenados
â””â”€â”€ video/                 # VÃ­deos de demonstraÃ§Ã£o (se aplicÃ¡vel)
```

## ğŸ› ï¸ Requisitos

- Python 3.8+
- Gymnasium
- Stable Baselines3
- MLflow
- Numpy
- Matplotlib

Veja `requeriments.txt` para a lista completa de dependÃªncias.

## ğŸ“¦ InstalaÃ§Ã£o

1. Clone ou baixe este repositÃ³rio
2. Instale as dependÃªncias:

```bash
pip install -r requeriments.txt
```

3. Instale o pacote clÃ¡ssico do Gymnasium (se necessÃ¡rio):

```bash
pip install "gymnasium[classic-control]"
```

## ğŸš€ Como Usar

### Treinar o Agente

Execute o script de treinamento:

```bash
python train_cartpole.py
```

Este script irÃ¡:
- Criar um ambiente CartPole
- Treinar um modelo PPO por 100.000 timesteps
- Avaliar o modelo em 50 episÃ³dios
- Registrar mÃ©tricas no MLflow
- Salvar o modelo treinado

### DemonstraÃ§Ã£o com AÃ§Ã£o AleatÃ³ria

Para ver o ambiente funcionando com aÃ§Ãµes aleatÃ³rias:

```bash
python agent-cart.py
```

Este script executa episÃ³dios com aÃ§Ãµes aleatÃ³rias atÃ© atingir uma recompensa de 80 pontos.

## ğŸ“Š MÃ©tricas e Rastreamento

O projeto utiliza **MLflow** para rastrear:
- Algoritmo utilizado (PPO)
- NÃºmero de timesteps de treinamento
- NÃºmero de episÃ³dios de avaliaÃ§Ã£o
- Recompensa mÃ©dia (avg_reward)
- Recompensa mÃ¡xima (max_reward)
- Recompensa mÃ­nima (min_reward)

Visualize o histÃ³rico de execuÃ§Ãµes:

```bash
mlflow ui
```

EntÃ£o acesse `http://localhost:5000` no seu navegador.

## ğŸ® Ambiente CartPole

O CartPole Ã© um ambiente clÃ¡ssico de controle do Gymnasium onde:
- **Objetivo**: Manter o poste em pÃ©
- **AÃ§Ã£o**: Empurrar o carrinho para esquerda (0) ou direita (1)
- **Recompensa**: +1 para cada timestep enquanto o poste estÃ¡ em pÃ©
- **EpisÃ³dio termina quando**: O poste cai ou mÃ¡ximo de timesteps Ã© atingido

## ğŸ“ˆ Resultados Esperados

Um agente bem treinado deve atingir:
- Recompensa mÃ©dia: ~500+ pontos
- Recompensa mÃ¡xima: PrÃ³ximo ao mÃ¡ximo do ambiente

## ğŸ”§ PersonalizaÃ§Ã£o

Para modificar os parÃ¢metros de treinamento, edite `train_cartpole.py`:

```python
timesteps = 100_000      # NÃºmero de timesteps de treinamento
eval_episodes = 50       # NÃºmero de episÃ³dios para avaliaÃ§Ã£o
```

## ğŸ“š ReferÃªncias

- [Gymnasium Documentation](https://gymnasium.farama.org/)
- [Stable Baselines3](https://stable-baselines3.readthedocs.io/)
- [MLflow Tracking](https://mlflow.org/docs/latest/tracking.html)
- [CartPole Problem](https://gymnasium.farama.org/environments/classic_control/cart_pole/)

## ğŸ“ LicenÃ§a

Este projeto Ã© fornecido como estÃ¡ para fins educacionais.

## ğŸ‘¤ Autor

Projeto de aprendizado por reforÃ§o desenvolvido em Python.

---

**Ãšltima atualizaÃ§Ã£o**: Dezembro de 2025
